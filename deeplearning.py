# -*- coding: utf-8 -*-
"""DeepLearning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FPiapjK26mnBXAA3hCd3QZhIghxYxRYp
"""

!pip install transformers datasets scikit-learn torch -q

"""## Libraries"""

import torch
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer
from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import pandas as pd

"""## Data Processing"""

dataset_url = "https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv"
df = pd.read_csv(dataset_url, sep='\t', header=None, names=['label', 'text'])
df['label'] = df['label'].map({'ham':0, 'spam':1})
df

"""## Splitting Data into Train / Validation / Test

"""

train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['text'], df['label'], test_size=0.3, random_state=42)
val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)

val_dataset  = Dataset.from_dict({'text': val_texts.tolist(), 'label': val_labels.tolist()})

"""## Tokenization"""

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

def tokenize(batch):
    return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)

val_dataset = val_dataset.map(tokenize, batched=True)
val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

"""## Loading Base Model"""

# Base Model
base_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

"""## Evaluation Metrices"""

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}

"""## Testing Baseline in the data before fine tuning
This process is to get the results before and after
"""

trainer_baseline = Trainer(
    model=base_model,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

baseline_results = trainer_baseline.evaluate()
print("Baseline Validation Results (Before Fine-tuning):")
print(baseline_results)

"""## Fine-Tuning"""

from datasets import Dataset
from sklearn.model_selection import train_test_split
import pandas as pd

dataset_url = "https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv"
df = pd.read_csv(dataset_url, sep='\t', header=None, names=['label', 'text'])
df['label'] = df['label'].map({'ham':0, 'spam':1})

train_texts, temp_texts, train_labels, temp_labels = train_test_split(df['text'], df['label'], test_size=0.3, random_state=42)
val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)

train_dataset = Dataset.from_dict({'text': train_texts.tolist(), 'label': train_labels.tolist()})
val_dataset   = Dataset.from_dict({'text': val_texts.tolist(), 'label': val_labels.tolist()})
test_dataset  = Dataset.from_dict({'text': test_texts.tolist(), 'label': test_labels.tolist()})

from transformers import DistilBertTokenizerFast

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

def tokenize(batch):
    return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize, batched=True)
val_dataset   = val_dataset.map(tokenize, batched=True)
test_dataset  = test_dataset.map(tokenize, batched=True)

train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

for param in model.distilbert.parameters():
    param.requires_grad = False

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    do_train=True,
    do_eval=True,
    logging_dir='./logs',
    learning_rate=5e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    save_strategy="no"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

trainer.train()

results = trainer.evaluate(test_dataset)
print("Test Set Results (After Fine-tuning):")
print(results)

import pandas as pd

results_df = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-score', 'Loss'],
    'Baseline': [
        baseline_results.get('eval_accuracy', 0),
        baseline_results.get('eval_precision', 0),
        baseline_results.get('eval_recall', 0),
        baseline_results.get('eval_f1', 0),
        baseline_results.get('eval_loss', 0)
    ],
    'Fine-tuned': [
        results.get('eval_accuracy', 0),
        results.get('eval_precision', 0),
        results.get('eval_recall', 0),
        results.get('eval_f1', 0),
        results.get('eval_loss', 0)
    ]
})

results_df

"""## Optimizer Improvements"""

model_path = "./spam-detector-model"
trainer.save_model(model_path)
tokenizer.save_pretrained(model_path)

print(f"Model Saved IN {model_path}")

from transformers import TrainingArguments, Trainer
import numpy as np

def run_optimizer_experiment(opt_name, lr, extra_kwargs={}):
    print(f"\n=== Running {opt_name} | lr={lr} ===")


    model_exp = DistilBertForSequenceClassification.from_pretrained(
        "./spam-detector-model"
    )


    for param in model_exp.distilbert.parameters():
        param.requires_grad = False

    training_args_exp = TrainingArguments(
        output_dir="./opt_results",
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        learning_rate=lr,
        logging_steps=20,
        save_strategy="no"
    )

    trainer_exp = Trainer(
        model=model_exp,
        args=training_args_exp,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
    )

    if opt_name == "AdamW":
        from torch.optim import AdamW
        optimizer = AdamW(model_exp.parameters(), lr=lr, **extra_kwargs)

    elif opt_name == "RMSProp":
        from torch.optim import RMSprop
        optimizer = RMSprop(model_exp.parameters(), lr=lr, **extra_kwargs)

    else:
        raise ValueError("Optimizer not supported")

    trainer_exp.optimizer = optimizer

    trainer_exp.train()
    results = trainer_exp.evaluate()
    print(results)
    return results

optimizer_results = {}

optimizer_results['AdamW_lr5e-5']   = run_optimizer_experiment("AdamW", 5e-5)

optimizer_results['AdamW_lr1e-4']   = run_optimizer_experiment("AdamW", 1e-4)

optimizer_results['RMSProp_lr5e-5'] = run_optimizer_experiment("RMSProp", 5e-5)

optimizer_results['RMSProp_lr1e-4'] = run_optimizer_experiment("RMSProp", 1e-4)

import pandas as pd

comparison_table = pd.DataFrame([
    {
        "Optimizer": name,
        "Accuracy": results.get("eval_accuracy", 0),
        "Precision": results.get("eval_precision", 0),
        "Recall": results.get("eval_recall", 0),
        "F1 Score": results.get("eval_f1", 0),
        "Loss": results.get("eval_loss", 0)
    }
    for name, results in optimizer_results.items()
])

print(comparison_table)

"""## Learning Rate improvements"""

def run_lr_experiment(lr_value):
    print(f"\n=== Running AdamW with Learning Rate = {lr_value} ===")


    model_lr = DistilBertForSequenceClassification.from_pretrained(
        "./spam-detector-model"
    )


    for param in model_lr.distilbert.parameters():
        param.requires_grad = False

    training_args_lr = TrainingArguments(
        output_dir="./lr_results",
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        learning_rate=lr_value,
        logging_steps=20,
        save_strategy="no"
    )

    trainer_lr = Trainer(
        model=model_lr,
        args=training_args_lr,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics
    )


    from torch.optim import AdamW
    optimizer = AdamW(model_lr.parameters(), lr=lr_value)
    trainer_lr.optimizer = optimizer

    trainer_lr.train()
    results = trainer_lr.evaluate()
    print(results)
    return results



lr_results = {}

lr_results["AdamW_lr3e-5"] = run_lr_experiment(3e-5)

lr_results["AdamW_lr1e-5"] = run_lr_experiment(1e-5)

lr_table = pd.DataFrame([
    {
        "Learning Rate": name,
        "Accuracy": results.get("eval_accuracy", 0),
        "Precision": results.get("eval_precision", 0),
        "Recall": results.get("eval_recall", 0),
        "F1 Score": results.get("eval_f1", 0),
        "Loss": results.get("eval_loss", 0)
    }
    for name, results in lr_results.items()
])

print("\n=== Learning Rate Comparison Table ===\n")
print(lr_table)

"""## Data Augmentation improvement"""

!pip install nlpaug -q
!pip install nltk -q

import nlpaug.augmenter.word as naw
import nltk
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger_eng')

from datasets import Dataset

syn_aug = naw.SynonymAug(aug_src='wordnet')

aug_texts = []
aug_labels = []

for text, label in zip(train_texts[:300], train_labels[:300]):
    augmented = syn_aug.augment(text)

    if isinstance(augmented, list):
        augmented = ' '.join(augmented)
    aug_texts.append(augmented)
    aug_labels.append(label)


final_train_texts  = list(train_texts) + aug_texts
final_train_labels = list(train_labels) + aug_labels


train_dataset_aug = Dataset.from_dict({
    'text': final_train_texts,
    'label': final_train_labels
})


train_dataset_aug = train_dataset_aug.map(tokenize, batched=True)
train_dataset_aug.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

print("✅ Synonym Augmentation Dataset created successfully!")
print(f"Original train size: {len(train_texts)}, Augmented train size: {len(final_train_texts)}")

from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments

model_aug1 = DistilBertForSequenceClassification.from_pretrained(
    "./spam-detector-model",
    num_labels=2
)

for param in model_aug1.distilbert.parameters():
    param.requires_grad = False

training_args_aug = TrainingArguments(
    output_dir='./aug_results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=5e-5,
    logging_dir='./logs',
    do_train=True,
    do_eval=True,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    save_strategy="no"
)

trainer_aug1 = Trainer(
    model=model_aug1,
    args=training_args_aug,
    train_dataset=train_dataset_aug,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

trainer_aug1.train()

aug1_results = trainer_aug1.evaluate(test_dataset)

print("✅ Synonym Augmentation Results on Test Set:")
print(aug1_results)

"""## Dropout"""

from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments


model_dropout = DistilBertForSequenceClassification.from_pretrained(
    "./spam-detector-model",
    num_labels=2
)

for param in model_dropout.distilbert.parameters():
    param.requires_grad = False


model_dropout.dropout = torch.nn.Dropout(p=0.3)


training_args_dropout = TrainingArguments(
    output_dir='./dropout_results',
    num_train_epochs=5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=5e-5,
    logging_dir='./logs',
    do_train=True,
    do_eval=True,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    save_strategy="no"
)


trainer_dropout = Trainer(
    model=model_dropout,
    args=training_args_dropout,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)


trainer_dropout.train()


dropout_results = trainer_dropout.evaluate(test_dataset)
print("Dropout Modification Results (p=0.3) on Test Set:")
print(dropout_results)

# Testing on New Data.

"""## Transfer Learning"""

!pip install transformers datasets scikit-learn torch -q

import pandas as pd
from datasets import Dataset
from sklearn.model_selection import train_test_split
import torch
from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import requests, zipfile, io

dataset_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"

r = requests.get(dataset_url)
z = zipfile.ZipFile(io.BytesIO(r.content))
df_new = pd.read_csv(z.open('SMSSpamCollection'), sep='\t', header=None, names=['label','text'])
df_new['label'] = df_new['label'].map({'ham':0, 'spam':1})
print(f"New dataset size: {len(df_new)}")
df_new.tail()

val_texts_new, test_texts_new, val_labels_new, test_labels_new = train_test_split(
    df_new['text'], df_new['label'], test_size=0.5, random_state=42
)

val_dataset_new = Dataset.from_dict({'text': val_texts_new.tolist(), 'label': val_labels_new.tolist()})
test_dataset_new = Dataset.from_dict({'text': test_texts_new.tolist(), 'label': test_labels_new.tolist()})

tokenizer = DistilBertTokenizerFast.from_pretrained("./spam-detector-model")

def tokenize(batch):
    return tokenizer(batch['text'], padding="max_length", truncation=True, max_length=128)

val_dataset_new = val_dataset_new.map(tokenize, batched=True)
test_dataset_new = test_dataset_new.map(tokenize, batched=True)

val_dataset_new.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset_new.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {'accuracy': acc, 'precision': precision, 'recall': recall, 'f1': f1}

model_new = DistilBertForSequenceClassification.from_pretrained("./spam-detector-model")
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

for param in model_new.distilbert.parameters():
    param.requires_grad = False

training_args_new = TrainingArguments(
    output_dir='./fine_tune_new',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=5e-5,
    logging_dir='./logs_new',
    do_train=True,
    do_eval=True,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    save_strategy="no"
)

trainer_new = Trainer(
    model=model_new,
    args=training_args_new,
    train_dataset=val_dataset_new,
    eval_dataset=val_dataset_new,
    compute_metrics=compute_metrics,
    data_collator=data_collator
)

trainer_new.train()

results_new = trainer_new.evaluate(test_dataset_new)
print("Results on New Test Set:")
print(results_new)

import pandas as pd


results_new = trainer_new.evaluate(test_dataset_new)


results_table = pd.DataFrame({
    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-score', 'Loss'],
    'Value': [
        results_new.get('eval_accuracy', 0),
        results_new.get('eval_precision', 0),
        results_new.get('eval_recall', 0),
        results_new.get('eval_f1', 0),
        results_new.get('eval_loss', 0)
    ]
})

print("Results on New Test Set:")
print(results_table)