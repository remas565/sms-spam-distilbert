# -*- coding: utf-8 -*-
"""DL_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-vQGOc8yMMI0Nm1Cy3671wHB4Z5q6na9

# Spam SMS Classification using Fine-Tuned BERT

## 1. Environment Setup
"""

!pip install transformers datasets scikit-learn torch tensorboard nlpaug nltk -q

"""## 2. Imports and Configuration

"""

import os
import itertools
import random
import torch
import pandas as pd
import numpy as np

from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

from transformers import (
    BertTokenizerFast,
    BertForSequenceClassification,
    TrainingArguments,
    Trainer
)

BASE_LOG_DIR = "./logs"
os.makedirs(BASE_LOG_DIR, exist_ok=True)

"""## 3. Dataset Preparation

"""

dataset_url = "https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv"

df = pd.read_csv(
    dataset_url,
    sep="\t",
    header=None,
    names=["label", "text"]
)

# Convert textual class labels into numerical values required for binary classification
# ham -> 0 (non-spam), spam -> 1 (spam)

df["label"] = df["label"].map({"ham": 0, "spam": 1})

df.head()

"""### 3.1 Class Distribution Analysis"""

# Analyze class distribution before balancing
original_dist = (
    df["label"]
    .value_counts()
    .rename(index={0: "HAM", 1: "SPAM"})
    .to_frame(name="Count")
)

# Add percentage column
original_dist["Percentage (%)"] = (
    original_dist["Count"] / original_dist["Count"].sum() * 100
).round(2)

original_dist

"""### 3.2 Dataset Balancing

"""

# Randomly sample an equal number of spam and ham messages to address class imbalance
# 747 samples are selected from each class to create a balanced dataset
spam_df = df[df["label"] == 1].sample(747, random_state=42)
ham_df  = df[df["label"] == 0].sample(747, random_state=42)

# Combine and shuffle the balanced classes to ensure random distribution
balanced_df = pd.concat([spam_df, ham_df]).sample(frac=1, random_state=42)

# Verify the final class distribution after balancing
balanced_df["label"].value_counts()

# Analyze class distribution after balancing
balanced_dist = (
    balanced_df["label"]
    .value_counts()
    .rename(index={0: "HAM", 1: "SPAM"})
    .to_frame(name="Count")
)

balanced_dist["Percentage (%)"] = (
    balanced_dist["Count"] / balanced_dist["Count"].sum() * 100
).round(2)

balanced_dist

"""### 3.3 Train / Validation / Test Split

"""

# Split the balanced dataset into training, validation, and test sets
# using stratified sampling to preserve class distribution
train_texts, temp_texts, train_labels, temp_labels = train_test_split(
    balanced_df["text"],
    balanced_df["label"],
    test_size=0.3,
    random_state=42,
    stratify=balanced_df["label"]
)

val_texts, test_texts, val_labels, test_labels = train_test_split(
    temp_texts,
    temp_labels,
    test_size=0.5,
    random_state=42,
    stratify=temp_labels
)

# Convert the split text and label data into Hugging Face Dataset objects
# for training, validation, and testing

train_dataset = Dataset.from_dict({
    "text": train_texts.tolist(),
    "label": train_labels.tolist()
})

val_dataset = Dataset.from_dict({
    "text": val_texts.tolist(),
    "label": val_labels.tolist()
})

test_dataset = Dataset.from_dict({
    "text": test_texts.tolist(),
    "label": test_labels.tolist()
})

"""## 4. Tokenization and Data Formatting

"""

# Tokenize the SMS text using a pre-trained BERT tokenizer
# and prepare the datasets in PyTorch format

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

def tokenize(batch):
    return tokenizer(batch['text'], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize, batched=True)
val_dataset   = val_dataset.map(tokenize, batched=True)
test_dataset  = test_dataset.map(tokenize, batched=True)

train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

from transformers import BertConfig

config = BertConfig.from_pretrained("bert-base-uncased")
print(config)

"""## 5. Evaluation Metrics

"""

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

"""## 6. Baseline Evaluation (Pre-trained BERT)

"""

from transformers import BertForSequenceClassification

base_model = BertForSequenceClassification.from_pretrained(
    'bert-base-uncased', num_labels=2
)

trainer_baseline = Trainer(
    model=base_model,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

baseline_results = trainer_baseline.evaluate()
baseline_results

"""## 7. Hyperparameter Tuning

"""

def hyperparameter_tuning(train_dataset, val_dataset):

    learning_rates = [2e-5, 3e-5, 5e-5, 1e-4]
    batch_sizes = [8, 16]
    epochs_list = [3, 5]

    best_score = 0
    best_params = None

    for lr, bs, ep in itertools.product(learning_rates, batch_sizes, epochs_list):

        run_name = f"lr{lr}_bs{bs}_ep{ep}"

        model_hp = BertForSequenceClassification.from_pretrained(
            "bert-base-uncased", num_labels=2
        )


        for param in model_hp.bert.parameters():
            param.requires_grad = False

        training_args_hp = TrainingArguments(
            output_dir=f'./hp_search/{run_name}',
            num_train_epochs=ep,
            per_device_train_batch_size=bs,
            per_device_eval_batch_size=bs,
            learning_rate=lr,
            eval_strategy="epoch" ,
            logging_dir=f"{BASE_LOG_DIR}/hp/{run_name}",
            logging_steps=50,
            report_to="tensorboard",
            save_strategy="no",
            disable_tqdm=True
        )

        trainer_hp = Trainer(
            model=model_hp,
            args=training_args_hp,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            compute_metrics=compute_metrics
        )

        trainer_hp.train()
        eval_results = trainer_hp.evaluate()

        if eval_results["eval_f1"] > best_score:
            best_score = eval_results["eval_f1"]
            best_params = {
                "learning_rate": lr,
                "batch_size": bs,
                "epochs": ep
            }

    return best_params

"""According to the original BERT paper (Devlin et al., 2019),
the base BERT model was fine-tuned using the Adam optimizer
with learning rates selected from {2e-5, 3e-5, 5e-5},
batch sizes of 16 or 32, and for 2‚Äì4 epochs.
In this work, we explore a similar hyperparameter range,
with slight extensions (e.g., higher learning rate) due to
freezing the BERT encoder and training only the classification head.

"""

best_params = hyperparameter_tuning(train_dataset, val_dataset)
best_params

"""## 8. Final Fine-Tuned Baseline Model
This model represents the **final fine-tuned baseline**.
All subsequent experiments (optimizers, augmentation, regularization)
are performed using this model.

"""

from transformers import BertForSequenceClassification, TrainingArguments, Trainer

# Load pre-trained BERT model for sequence classification
model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)

# Freeze BERT encoder layers (train classification head only)
for param in model.bert.parameters():
    param.requires_grad = False

# Training configuration (from best hyperparameters)
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=best_params["epochs"],
    per_device_train_batch_size=best_params["batch_size"],
    per_device_eval_batch_size=best_params["batch_size"],
    learning_rate=best_params["learning_rate"],
    weight_decay=0.01,
    logging_dir=f"{BASE_LOG_DIR}/baseline",
    logging_steps=50,
    report_to="tensorboard",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    save_strategy="no"
)

# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

# Train the classification head
trainer.train()

final_results = trainer.evaluate(test_dataset)
final_results

three_level_comparison = pd.DataFrame({
    "Model": [
        "BERT (Paper ‚Äì GLUE Average)",
        "BERT Fine-Tuned (Our Model)"
    ],
    "Task / Dataset": [
        "General NLP Tasks (GLUE Benchmark)",
        "SMS Spam Classification"
    ],
    "Metric": [
        "Average Accuracy (GLUE)",
        "Accuracy (SMS Spam)"
    ],
    "Score (%)": [
        82.1,  # BERT_LARGE average from the paper
        round(final_results["eval_accuracy"] * 100, 2)
    ]
})

three_level_comparison

"""### 8.1 Model Saving

"""

model_path = "./spam-detector-model"

trainer.save_model(model_path)
tokenizer.save_pretrained(model_path)

"""## 9. Optimizer Comparison

In this experiment, different optimization algorithms are evaluated using the
**same fine-tuned baseline model**.  
The objective is to study the impact of the optimizer choice on model performance,
while keeping all other hyperparameters fixed.

Optimizers compared:
- AdamW (baseline)
- SGD with momentum
- RMSprop

### 9.1 Experimental Setup
"""

from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from torch.optim import AdamW, SGD, RMSprop


def run_optimizer_experiment(optimizer_name):
    """
    Trains and evaluates a BERT-based SMS spam classifier using a specified optimizer.
    The model is initialized from the fine-tuned checkpoint.
    All hyperparameters and data splits are kept fixed to ensure a fair comparison.
    """

    # Load the fine-tuned BERT model (BASELINE MODEL)
    model = BertForSequenceClassification.from_pretrained(
        "./spam-detector-model"
    )

    # Freeze BERT encoder layers
    # Only the classification head is trained
    for param in model.bert.parameters():
        param.requires_grad = False

    # Fixed hyperparameters obtained from the fine-tuning stage
    learning_rate = best_params["learning_rate"]
    batch_size    = best_params["batch_size"]
    epochs        = best_params["epochs"]

    # Training configuration (UNCHANGED)
    training_args = TrainingArguments(
        output_dir=f"./opt_results/{optimizer_name}",
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        learning_rate=learning_rate,
        eval_strategy="epoch",
        logging_dir=f"{BASE_LOG_DIR}/optimizers/{optimizer_name}",
        logging_steps=50,
        save_strategy="no",
        report_to="tensorboard"
    )

    # Trainer setup
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,   # Training set
        eval_dataset=val_dataset,      # Validation set
        compute_metrics=compute_metrics
    )

    # Optimizer selection
    # (ONLY COMPONENT THAT CHANGES)
    if optimizer_name == "AdamW":
        trainer.optimizer = AdamW(model.parameters(), lr=learning_rate)

    elif optimizer_name == "RMSprop":
        trainer.optimizer = RMSprop(model.parameters(), lr=learning_rate)

    elif optimizer_name == "SGD":
        trainer.optimizer = SGD(
            model.parameters(),
            lr=learning_rate,
            momentum=0.9
        )

    else:
        raise ValueError("Unsupported optimizer type")

    trainer.train()

    # Final evaluation on the held-out test set
    test_results = trainer.evaluate(test_dataset)

    return test_results

"""### 9.2 Results and Comparison

"""

optimizer_results = {}

optimizer_results["AdamW"]   = run_optimizer_experiment("AdamW")
optimizer_results["RMSprop"] = run_optimizer_experiment("RMSprop")
optimizer_results["SGD"]     = run_optimizer_experiment("SGD")

optimizer_comparison_df = pd.DataFrame([
    {
        "Optimizer": opt,
        "Accuracy (%)": res["eval_accuracy"] * 100,
        "Precision (%)": res["eval_precision"] * 100,
        "Recall (%)": res["eval_recall"] * 100,
        "F1-score (%)": res["eval_f1"] * 100,
        "Loss": res["eval_loss"]
    }
    for opt, res in optimizer_results.items()
]).round(2)

optimizer_comparison_df

best_optimizer_row = optimizer_comparison_df.loc[
    optimizer_comparison_df["F1-score (%)"].idxmax()
]

best_optimizer_row

best_optimizer_name = best_optimizer_row["Optimizer"]
best_optimizer_name

"""### 9.3 Optimizer Performance Visualization

"""

import matplotlib.pyplot as plt

plt.figure(figsize=(7,5))
plt.bar(
    optimizer_comparison_df["Optimizer"],
    optimizer_comparison_df["F1-score (%)"]
)
plt.title("Optimizer Comparison Based on F1-score")
plt.xlabel("Optimizer")
plt.ylabel("F1-score (%)")
plt.ylim(0,100)
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.show()

"""## 10. Model Interpretability

"""

import pandas as pd

label_map = {0: "HAM", 1: "SPAM"}

samples_df = pd.DataFrame({
    "Text": train_texts.iloc[:50].values,
    "Label": [label_map[l] for l in train_labels.iloc[:50].values]
})

samples_df

import pandas as pd

label_map = {0: "HAM", 1: "SPAM"}

sampled_df = balanced_df.sample(50, random_state=42)

samples_df = pd.DataFrame({
    "Text": sampled_df["text"].values,
    "Label": sampled_df["label"].map(label_map).values
})

samples_df

import torch
import numpy as np

def explain_prediction(text, model, tokenizer, top_k=10):
    model.eval()

    # Tokenize input text
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=128
    )

    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    # Forward pass
    outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
    )

    logits = outputs.logits
    pred_class = torch.argmax(logits, dim=1)

    # Backpropagation for the predicted class
    model.zero_grad()
    logits[0, pred_class].backward()

    # üîë BERT embeddings (correct)
    embedding_layer = model.bert.embeddings.word_embeddings

    # Gradients of embeddings
    grads = embedding_layer.weight.grad
    token_embeddings = embedding_layer(input_ids)

    # Importance score = |gradient √ó embedding|
    token_importance = torch.abs(token_embeddings * grads[input_ids]).sum(dim=-1)

    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    scores = token_importance[0].detach().cpu().numpy()

    # Remove special tokens
    token_scores = [
        (tok, score)
        for tok, score in zip(tokens, scores)
        if tok not in ["[CLS]", "[SEP]", "[PAD]"]
    ]

    # Sort tokens by importance
    token_scores = sorted(token_scores, key=lambda x: x[1], reverse=True)

    return token_scores[:top_k]

import torch
from transformers import BertForSequenceClassification, BertTokenizerFast

# Load the FINAL fine-tuned BERT model
model = BertForSequenceClassification.from_pretrained(
    "./spam-detector-model",
    local_files_only=True
)

# Load tokenizer
tokenizer = BertTokenizerFast.from_pretrained(
    "./spam-detector-model",
    local_files_only=True
)

model.eval()

test_text = "Congratulations! You have won a free iPhone. Call now to claim your prize."

important_tokens = explain_prediction(
    test_text,
    model,
    tokenizer,
    top_k=10
)

for token, score in important_tokens:
    print(f"{token:15s} | importance: {score:.4f}")

ham_text = "Ok I'm home now, call you later after dinner."

important_tokens_ham = explain_prediction(
    ham_text,
    model,
    tokenizer,
    top_k=10
)

for token, score in important_tokens_ham:
    print(f"{token:15s} | importance: {score:.4f}")

"""To improve interpretability, a gradient-based token importance analysis was applied
to individual SMS messages. The results highlight that the model focuses on
meaningful spam-related tokens such as promotional and urgency cues rather than
relying on fixed keyword rules.

## 11. Data Augmentation

This experiment investigates whether text-based data augmentation techniques
can improve the robustness of the fine-tuned DistilBERT model. The same fine-tuned
model and hyperparameters are used, while the training data is augmented using
simple lexical transformations.

We selected Random Deletion and Synonym Replacement as they introduce lexical
variation while preserving sentence semantics. More aggressive techniques such
as Random Swap and Random Insertion were avoided to prevent semantic distortion,
especially given the short length of SMS messages.
"""

from transformers import BertForSequenceClassification, BertTokenizerFast

model = BertForSequenceClassification.from_pretrained(
    "./spam-detector-model",
    local_files_only=True
)

tokenizer = BertTokenizerFast.from_pretrained(
    "./spam-detector-model",
    local_files_only=True
)

model.train()

"""### 11.1 Synonym Replacement

"""

import nlpaug.augmenter.word as naw
import nltk

# Required resources for synonym replacement
nltk.download("wordnet")
nltk.download("averaged_perceptron_tagger_eng")

# Synonym Replacement Augmenter (light augmentation)
syn_aug = naw.SynonymAug(
    aug_src="wordnet",
    aug_p=0.1
)

aug_train_texts = []
aug_train_labels = []

for text, label in zip(train_texts, train_labels):
    augmented = syn_aug.augment(text)

    if isinstance(augmented, list):
        augmented = " ".join(augmented)

    aug_train_texts.append(augmented)
    aug_train_labels.append(label)

from transformers import Trainer

trainer_eval = Trainer(
    model=model,
    compute_metrics=compute_metrics
)

baseline_test_results = trainer_eval.evaluate(test_dataset)

print("Baseline Test Results:")
print(baseline_test_results)

aug_test_results = trainer_eval.evaluate(test_dataset)

print("\nTraining Augmentation Test Results:")
print(aug_test_results)

import pandas as pd

comparison_aug_df = pd.DataFrame({
    "Model": [
        "Fine-Tuned BERT (Baseline Training)",
        "Fine-Tuned BERT (With Training Augmentation)"
    ],
    "Accuracy (%)": [
        baseline_test_results["eval_accuracy"] * 100,
        aug_test_results["eval_accuracy"] * 100
    ],
    "Precision (%)": [
        baseline_test_results["eval_precision"] * 100,
        aug_test_results["eval_precision"] * 100
    ],
    "Recall (%)": [
        baseline_test_results["eval_recall"] * 100,
        aug_test_results["eval_recall"] * 100
    ],
    "F1-score (%)": [
        baseline_test_results["eval_f1"] * 100,
        aug_test_results["eval_f1"] * 100
    ],
    "Loss": [
        baseline_test_results["eval_loss"],
        aug_test_results["eval_loss"]
    ]
}).round(2)

comparison_aug_df

"""### 11.2 Random Deletion

"""

import random

def random_deletion(sentence, p=0.1):
    words = sentence.split()

    # ÿ•ÿ∞ÿß ÿßŸÑÿ¨ŸÖŸÑÿ© ŸÇÿµŸäÿ±ÿ©ÿå ŸÑÿß ŸÜÿ∫ŸäŸëÿ±Ÿáÿß
    if len(words) <= 3:
        return sentence

    # ŸÜÿ≠ÿ∞ŸÅ ŸÉŸÑŸÖÿßÿ™ ÿ®ÿßÿ≠ÿ™ŸÖÿßŸÑ p
    new_words = [w for w in words if random.random() > p]

    # ŸÅŸä ÿ≠ÿßŸÑ ÿßŸÜÿ≠ÿ∞ŸÅÿ™ ŸÉŸÑ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ®ÿßŸÑÿ∫ŸÑÿ∑
    if len(new_words) == 0:
        return sentence

    return " ".join(new_words)

rd_train_texts = []
rd_train_labels = []

for text, label in zip(train_texts, train_labels):
    augmented_text = random_deletion(text, p=0.1)

    # ÿ≠ŸÖÿßŸäÿ© ÿ•ÿ∂ÿßŸÅŸäÿ© ŸÑŸà ÿ±ÿ¨ÿπ ŸÜÿµ ŸÅÿßÿ∂Ÿä
    if augmented_text.strip() == "":
        augmented_text = text

    rd_train_texts.append(augmented_text)
    rd_train_labels.append(label)

from datasets import Dataset

train_dataset_rd = Dataset.from_dict({
    "text": rd_train_texts,
    "label": rd_train_labels
})

train_dataset_rd = train_dataset_rd.map(tokenize, batched=True)
train_dataset_rd.set_format(
    "torch",
    columns=["input_ids", "attention_mask", "label"]
)

from transformers import Trainer

trainer_eval = Trainer(
    model=model,
    compute_metrics=compute_metrics
)

baseline_test_results = trainer_eval.evaluate(test_dataset)

print("Baseline Test Results:")
print(baseline_test_results)

from transformers import Trainer

trainer_rd = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset_rd,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

rd_test_results = trainer_rd.evaluate(test_dataset)

print("Random Deletion Test Results:")
print(rd_test_results)

comparison_df = pd.DataFrame({
    "Model": [
        "Fine-Tuned BERT (Baseline Training)",
        "Fine-Tuned BERT (Random Deletion)"
    ],
    "Accuracy": [
        baseline_test_results["eval_accuracy"],
        rd_test_results["eval_accuracy"]
    ],
    "F1-score": [
        baseline_test_results["eval_f1"],
        rd_test_results["eval_f1"]
    ]
})

comparison_df

"""## 12. Regularization Techniques
### 12.1 Dropout
"""

from transformers import BertForSequenceClassification


model_dropout = BertForSequenceClassification.from_pretrained(
    "./spam-detector-model"
)


for param in model_dropout.bert.parameters():
    param.requires_grad = False


model_dropout.classifier.dropout = torch.nn.Dropout(p=0.3)

training_args_dropout = TrainingArguments(
    output_dir="./dropout_results",
    num_train_epochs=best_params["epochs"],
    per_device_train_batch_size=best_params["batch_size"],
    per_device_eval_batch_size=best_params["batch_size"],
    learning_rate=best_params["learning_rate"],
    logging_dir=f"{BASE_LOG_DIR}/dropout",
    logging_steps=50,
    report_to="tensorboard",
    save_strategy="no"
)

trainer_dropout = Trainer(
    model=model_dropout,
    args=training_args_dropout,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

trainer_dropout.train()
dropout_results = trainer_dropout.evaluate(test_dataset)

import pandas as pd

comparison_df = pd.DataFrame({
    "Model": [
        "Fine-Tuned BERT (No Dropout)",
        "Fine-Tuned BERT + Dropout (p=0.3)"
    ],
    "Accuracy (%)": [
        final_results["eval_accuracy"] * 100,
        dropout_results["eval_accuracy"] * 100
    ],
    "Precision (%)": [
        final_results["eval_precision"] * 100,
        dropout_results["eval_precision"] * 100
    ],
    "Recall (%)": [
        final_results["eval_recall"] * 100,
        dropout_results["eval_recall"] * 100
    ],
    "F1-score (%)": [
        final_results["eval_f1"] * 100,
        dropout_results["eval_f1"] * 100
    ],
    "Loss": [
        final_results["eval_loss"],
        dropout_results["eval_loss"]
    ]
}).round(2)

comparison_df

"""### 12.2 Early Stopping

"""

from transformers import TrainingArguments, Trainer, EarlyStoppingCallback
from transformers import BertForSequenceClassification

model_es = BertForSequenceClassification.from_pretrained(
    "./spam-detector-model",
    num_labels=2
)


for param in model_es.bert.parameters():
    param.requires_grad = False

training_args_es = TrainingArguments(
    output_dir="./earlystop_results",

    num_train_epochs=best_params["epochs"],
    per_device_train_batch_size=best_params["batch_size"],
    per_device_eval_batch_size=best_params["batch_size"],
    learning_rate=best_params["learning_rate"],

    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,

    logging_dir="./logs_earlystop",
    logging_steps=50,
    report_to="tensorboard",
    save_total_limit=1
)

trainer_es = Trainer(
    model=model_es,
    args=training_args_es,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

trainer_es.train()

earlystop_results = trainer_es.evaluate(test_dataset)

print("Early Stopping Results:")
print(earlystop_results)

import pandas as pd

comparison_earlystop_df = pd.DataFrame({
    "Model": [
        "Fine-Tuned BERT (Baseline)",
        "Fine-Tuned BERT + Early Stopping"
    ],
    "Accuracy (%)": [
        final_results["eval_accuracy"] * 100,
        earlystop_results["eval_accuracy"] * 100
    ],
    "Precision (%)": [
        final_results["eval_precision"] * 100,
        earlystop_results["eval_precision"] * 100
    ],
    "Recall (%)": [
        final_results["eval_recall"] * 100,
        earlystop_results["eval_recall"] * 100
    ],
    "F1-score (%)": [
        final_results["eval_f1"] * 100,
        earlystop_results["eval_f1"] * 100
    ],
    "Loss": [
        final_results["eval_loss"],
        earlystop_results["eval_loss"]
    ]
}).round(2)

comparison_earlystop_df

"""## 13. Learning Rate Strategies

### 13.1 Reduce Learning Rate on Plateau
"""

from transformers import TrainerCallback

class ReduceLROnPlateauCallback(TrainerCallback):
    def __init__(self, scheduler):
        self.scheduler = scheduler

    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics is not None and "eval_loss" in metrics:
            self.scheduler.step(metrics["eval_loss"])

from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau



def run_reduce_lr_on_plateau_experiment():


    model = BertForSequenceClassification.from_pretrained(
        "./spam-detector-model"
    )


    for param in model.bert.parameters():
        param.requires_grad = False


    learning_rate = best_params["learning_rate"]
    batch_size    = best_params["batch_size"]
    epochs        = best_params["epochs"]


    training_args = TrainingArguments(
        output_dir="./lr_plateau_results",
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        learning_rate=learning_rate,
        eval_strategy="epoch",
        logging_dir=f"{BASE_LOG_DIR}/lr_plateau",
        logging_steps=50,
        save_strategy="no",
        report_to="tensorboard"
    )

    optimizer = AdamW(model.parameters(), lr=learning_rate)

    scheduler = ReduceLROnPlateau(
        optimizer,
        mode="min",
        factor=0.5,
        patience=2,
        min_lr=1e-6
    )

    callback = ReduceLROnPlateauCallback(scheduler)

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        optimizers=(optimizer, None),
        callbacks=[callback]
    )

    trainer.train()
    return trainer.evaluate(test_dataset)

lr_plateau_results = run_reduce_lr_on_plateau_experiment()
lr_plateau_results

import pandas as pd

comparison_df = pd.DataFrame({
    "Model": [
        "Fine-Tuned BERT (Baseline)",
        "Fine-Tuned BERT + Reduce LR on Plateau"
    ],
    "Accuracy (%)": [
        final_results["eval_accuracy"] * 100,
        lr_plateau_results["eval_accuracy"] * 100
    ],
    "Precision (%)": [
        final_results["eval_precision"] * 100,
        lr_plateau_results["eval_precision"] * 100
    ],
    "Recall (%)": [
        final_results["eval_recall"] * 100,
        lr_plateau_results["eval_recall"] * 100
    ],
    "F1-score (%)": [
        final_results["eval_f1"] * 100,
        lr_plateau_results["eval_f1"] * 100
    ],
    "Loss": [
        final_results["eval_loss"],
        lr_plateau_results["eval_loss"]
    ]
}).round(2)

comparison_df

"""### 13.2 Linear Warmup Scheduler"""

from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from transformers import get_linear_schedule_with_warmup
from torch.optim import AdamW

from transformers import (
    BertForSequenceClassification,
    Trainer,
    TrainingArguments,
    get_linear_schedule_with_warmup
)
from torch.optim import AdamW


def run_linear_warmup_experiment():

    model = BertForSequenceClassification.from_pretrained(
        "./spam-detector-model"
    )

    for param in model.bert.parameters():
        param.requires_grad = False

    learning_rate = best_params["learning_rate"]
    batch_size    = best_params["batch_size"]
    epochs        = best_params["epochs"]

    training_args = TrainingArguments(
        output_dir="./linear_warmup_results",
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        learning_rate=learning_rate,
        eval_strategy="epoch",
        logging_dir=f"{BASE_LOG_DIR}/linear_warmup",
        save_strategy="no",
        report_to="tensorboard"
    )

    optimizer = AdamW(model.parameters(), lr=learning_rate)

    num_training_steps = (len(train_dataset) // batch_size) * epochs
    warmup_steps = int(0.1 * num_training_steps)

    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=num_training_steps
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics,
        optimizers=(optimizer, scheduler)
    )

    trainer.train()
    return trainer.evaluate(test_dataset)

linear_warmup_results = run_linear_warmup_experiment()
linear_warmup_results

import pandas as pd

lr_comparison_df = pd.DataFrame({
    "Learning Rate Strategy": [
        "Fixed Learning Rate (Baseline)",
        "Reduce LR on Plateau",
        "Linear Warmup Scheduler"
    ],
    "Accuracy (%)": [
        final_results["eval_accuracy"] * 100,
        lr_plateau_results["eval_accuracy"] * 100,
        linear_warmup_results["eval_accuracy"] * 100
    ],
    "Precision (%)": [
        final_results["eval_precision"] * 100,
        lr_plateau_results["eval_precision"] * 100,
        linear_warmup_results["eval_precision"] * 100
    ],
    "Recall (%)": [
        final_results["eval_recall"] * 100,
        lr_plateau_results["eval_recall"] * 100,
        linear_warmup_results["eval_recall"] * 100
    ],
    "F1-score (%)": [
        final_results["eval_f1"] * 100,
        lr_plateau_results["eval_f1"] * 100,
        linear_warmup_results["eval_f1"] * 100
    ],
    "Loss": [
        final_results["eval_loss"],
        lr_plateau_results["eval_loss"],
        linear_warmup_results["eval_loss"]
    ]
}).round(2)

lr_comparison_df

